---
title: "Homework 3"
author: "[Alvaro Tapia]{style='background-color: yellow;'}"
toc: true
title-block-banner: true
title-block-style: default
#format: html
format: pdf
---

[Link to the Github repository](https://github.com/abt5572/hw-3-abt5572)

::: {.callout-note}
## Please Read

I'm using another repository for my codespaces because of an issue I have with github and the storage, so there will be no pdf or html commited in the link provided
:::

---

::: {.callout-important style="font-size: 0.8em;"}
## Due: Thu, Mar 2, 2023 @ 11:59pm

Please read the instructions carefully before submitting your assignment.

1. This assignment requires you to only upload a `PDF` file on Canvas
1. Don't collapse any code cells before submitting. 
1. Remember to make sure all your code output is rendered properly before uploading your submission.

⚠️ Please add your name to the author information in the frontmatter before submitting your assignment ⚠️
:::

For this assignment, we will be using the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality) dataset from the UCI Machine Learning Repository. The dataset consists of red and white _vinho verde_ wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests

We will be using the following libraries:

```{R}
library(readr)
library(tidyr)
library(dplyr)
library(purrr)
library(car)
library(glmnet)
```

<br><br><br><br>
---

## Question 1
::: {.callout-tip}
## 50 points
Regression with categorical covariate and $t$-Test
:::

###### 1.1 (5 points)

Read the wine quality datasets from the specified URLs and store them in data frames `df1` and `df2`.

```{R}
url1 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv"
url2 <- "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"
df1 <- read.csv(url1, sep = ';')
df2 <- read.csv(url2, sep = ';')
```

---

###### 1.2 (5 points)

Perform the following tasks to prepare the data frame `df` for analysis:

1. Combine the two data frames into a single data frame `df`, adding a new column called `type` to indicate whether each row corresponds to white or red wine. 
1. Rename the columns of `df` to replace spaces with underscores
1. Remove the columns `fixed_acidity` and `free_sulfur_dioxide`
1. Convert the `type` column to a factor
1. Remove rows (if any) with missing values.


```{R}
#1
df1$type <- "white"
df2$type <- "red"
df <- bind_rows(df1,df2)
#2
colnames(df) <- gsub('\\.', '_', colnames(df))
#3
df <- df %>% select(-c(fixed_acidity, free_sulfur_dioxide))
#4
df$type <- factor(df$type)
#5
df <- df %>% drop_na()
head(df) #Viewing the new df
```

```{R}
dim(df) #Comparing results
```


Your output to `R dim(df)` should be
```
[1] 6497   11
```



---

###### 1.3 (20 points)

Recall from STAT 200, the method to compute the $t$ statistic for the the difference in means (with the equal variance assumption)

1. Using `df` compute the mean of `quality` for red and white wine separately, and then store the difference in means as a variable called `diff_mean`. 

2. Compute the pooled sample variance and store the value as a variable called `sp_squared`. 

3. Using `sp_squared` and `diff_mean`, compute the $t$ Statistic, and store its value in a variable called `t1`.


```{R}
#1
quality_w = df$quality[df$type == 'white']
quality_r = df$quality[df$type == 'red']

red_mean <- mean(quality_r)
white_mean <- mean(quality_w)
diff_mean <- abs(white_mean - red_mean)

#2
v_w <- var(quality_w)
v_r <- var(quality_r)

n_w <- length(quality_w)
n_r <- length(quality_r)

#3
sp_squared <- (((n_w - 1) * v_w) + ((n_r - 1)* v_r)) / (n_w + n_r - 2) #Using formula

t1 <- diff_mean/(sqrt(sp_squared * (1/n_w + 1/n_r)))
t1 #displaying t-statistic
```


---

###### 1.4 (10 points)

Equivalently, R has a function called `t.test()` which enables you to perform a two-sample $t$-Test without having to compute the pooled variance and difference in means. 

Perform a two-sample t-test to compare the quality of white and red wines using the `t.test()` function with the setting `var.equal=TRUE`. Store the t-statistic in `t2`.

```{R}
t_test <- t.test(quality_w, quality_r, var.equal = TRUE)
t2 <- t_test$statistic
t2
```

---

###### 1.5 (5 points)

Fit a linear regression model to predict `quality` from `type` using the `lm()` function, and extract the $t$-statistic for the `type` coefficient from the model summary. Store this $t$-statistic in `t3`.

```{R}
fit <- lm(quality ~ type, data = df)
t3 <- summary(fit)$coefficients[2, "t value"]
t3
```


---

###### 1.6  (5 points)

Print a vector containing the values of `t1`, `t2`, and `t3`. What can you conclude from this? Why?

```{R}
c(t1, t2, t3) 
```

It is possible to conclude that all the three methods used will give us the same answer, therefore, they are all valid and correct to use.



<br><br><br><br>
<br><br><br><br>
---

## Question 2
::: {.callout-tip}
## 25 points
Collinearity
:::


---

###### 2.1 (5 points)

Fit a linear regression model with all predictors against the response variable `quality`. Use the `broom::tidy()` function to print a summary of the fitted model. What can we conclude from the model summary?


```{R}
model <- lm(quality ~ ., data = df)
summary_m <- broom::tidy(summary(model))
summary_m
```

We can conclude that most of the $p$-values except for citric acid and sulfur dioxide, have very low $p$-values which means that are significant. While those excluded ones might not be neccesary to be considered as predictors of wines quality.


---

###### 2.2 (10 points)

Fit two **simple** linear regression models using `lm()`: one with only `citric_acid` as the predictor, and another with only `total_sulfur_dioxide` as the predictor. In both models, use `quality` as the response variable. How does your model summary compare to the summary from the previous question?


```{R}
model_citric <- lm(quality ~ citric_acid, data = df)
summary_c <- broom::tidy(summary(model_citric))
summary_c
```

```{R}
model_sulfur <- lm(quality ~ total_sulfur_dioxide, data = df)
summary_s <- broom::tidy(summary(model_sulfur))
summary_s
```


The main comparision between the models is that in this case it is possible to appreciate that in these new models the citric acid and sulfur dioxide $p$-values are way lower than in the orignal model which means that now they are significant and therefore can be cosidered as potential predictors.


---

###### 2.3 (5 points)

Visualize the correlation matrix of all numeric columns in `df` using `corrplot()`

```{R}
library(corrplot)

df_cor <- df %>% 
  keep(is.numeric) %>%
  cor()
corrplot(df_cor, type = "upper")
```

---

###### 2.4 (5 points)

Compute the variance inflation factor (VIF) for each predictor in the full model using `vif()` function. What can we conclude from this?


```R
vif(model) # retrieved from previous calculation
```

It is possible to conclude that now we have very high inflation factors which shows a high correlation of predictors with the variables in the model (predictors that satisfy this: density, alcohol, type, and residual sugar). For the other predictors, they have a low variance inflation factor which tells us that they might not change based on other variables. 


<br><br><br><br>
<br><br><br><br>
---

## Question 3
::: {.callout-tip}
## 40 points

Variable selection
:::


---

###### 3.1 (5 points)

Run a backward stepwise regression using a `full_model` object as the starting model. Store the final formula in an object called `backward_formula` using the built-in `formula()` function in R

```{R}
full_model <- lm(quality ~ ., data = df)

backward_formula <- step(full_model, scope=formula(full_model), direction = "backward")
```

---

###### 3.2 (5 points)

Run a forward stepwise regression using a `null_model` object as the starting model. Store the final formula in an object called `forward_formula` using the built-in `formula()` function in R

```{R}
null_model <- lm(quality ~ 1, df)
forward_formula <- step(null_model, scope = formula(full_model), direction = "forward")
```



---

###### 3.3  (10 points)

1. Create a `y` vector that contains the response variable (`quality`) from the `df` dataframe. 

2. Create a design matrix `X` for the `full_model` object using the `make_model_matrix()` function provided in the Appendix. 

3. Then, use the `cv.glmnet()` function to perform LASSO and Ridge regression with `X` and `y`.

```{R}
#Retrieving the formula from appendix
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}

y <- df$quality 

lasso_reg <- cv.glmnet(x = make_model_matrix(forward_formula), y = y, alpha = 1)
lasso_reg

ridge_reg <- cv.glmnet(x = make_model_matrix(forward_formula), y = y, alpha = 0)
ridge_reg
```


Create side-by-side plots of the ridge and LASSO regression results. Interpret your main findings. 

```{R}
par(mfrow=c(1, 2))
plot(lasso_reg, main = "LASSO Regression")
plot(ridge_reg, main = "Ridge Regression")
```

Here we can appreciate that in the lasso plot the suggested value of log($\lambda$) is aproximately -3.3 with 4 varaibles and -7.5 with 8 variables for the optimal minimization of mean-squared error. On the other hand we hand we have that for the ridge regression plot, the ideal values are approximately -2.8 and -1.8 to minimize mean-squared error including all variables since there is no variable selection.



---

###### 3.4  (5 points)

Print the coefficient values for LASSO regression at the `lambda.1se` value? What are the variables selected by LASSO? 

Store the variable names with non-zero coefficients in `lasso_vars`, and create a formula object called `lasso_formula` using the `make_formula()` function provided in the Appendix. 

```{R}
#For the first question
lasso_coeff <- coef(lasso_reg, s = "lambda.1se")
lasso_coeff
```

The variables selected by LASSO are alcohol, volatile_acidity, sulphates, and residual_sugar.

```{R}
#For second question
#Retrieving the formula from Appendix
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}

lasso_vars <- rownames(lasso_coeff)[which(abs(lasso_coeff) > 0)][-1]
lasso_formula <- make_formula(lasso_vars)
lasso_formula
```


---

###### 3.5  (5 points)

Print the coefficient values for ridge regression at the `lambda.1se` value? What are the variables selected here? 

Store the variable names with non-zero coefficients in `ridge_vars`, and create a formula object called `ridge_formula` using the `make_formula()` function provided in the Appendix. 

```{R}
#For the first question
ridge_coeff <- coef(ridge_reg, s = "lambda.1se")
ridge_coeff
```

All variables are selected in this case for the ridge regression. 

```{R}
ridge_vars <- rownames(ridge_coeff)[which(abs(ridge_coeff) > 0)][-1]
ridge_formula <- make_formula(ridge_vars)
ridge_formula
```



---

###### 3.6  (10 points)

What is the difference between stepwise selection, LASSO and ridge based on you analyses above?

First of all, the lasso regression only selects 4 values (alcohol, volatile_acidity, sulphates, and residual_sugar) while the ridge regression uses all of them. With this it is possible to say that lasso tends to have variables coefficients to be 0 and they are eliminated from the final model while ridge doesn't. Also it is important to consider that lasso regression uses mean-squared error for variable selection and it works good with large datasets (large amount of columns). And ridge regression increases the reliability of the estimates.


<br><br><br><br>
<br><br><br><br>
---

## Question 4
::: {.callout-tip}
## 70 points

Variable selection
:::

---

###### 4.1  (5 points)

Excluding `quality` from `df` we have $10$ possible predictors as the covariates. How many different models can we create using any subset of these $10$ coavriates as possible predictors? Justify your answer. 

If we do the math we have that there are a total of 1024 possible models to be created in this case when using any subset from the $10$ covariates. The main reason is because there are different possible groupings of covariates to make a different model and this because for each model we can choose to include or exclude them (covariates).



---


###### 4.2  (20 points)

Store the names of the predictor variables (all columns except `quality`) in an object called `x_vars`.

```{R}
x_vars <- colnames(df %>% select(-quality))
x_vars #printing it
```

Use: 

* the `combn()` function (built-in R function) and 
* the `make_formula()` (provided in the Appendix) 

to **generate all possible linear regression formulas** using the variables in `x_vars`. This is most optimally achieved using the `map()` function from the `purrr` package.

```{R}
formulas <- map(
  1:length(x_vars),
  function(x){
    vars <- combn(x_vars, x, simplify = FALSE)
    map(vars, make_formula)
  }
) %>% unlist()
```

If your code is right the following command should return something along the lines of:

```{R}
sample(formulas, 4) %>% as.character()
# Output:
# [1] "quality ~ volatile_acidity + residual_sugar + density + pH + alcohol"                                                 
# [2] "quality ~ citric_acid"                                                                                                
# [3] "quality ~ volatile_acidity + citric_acid + residual_sugar + total_sulfur_dioxide + density + pH + sulphates + alcohol"
# [4] "quality ~ citric_acid + chlorides + total_sulfur_dioxide + pH + alcohol + type"  
```

---

###### 4.3  (10 points)
Use `map()` and `lm()` to fit a linear regression model to each formula in `formulas`, using `df` as the data source. Use `broom::glance()` to extract the model summary statistics, and bind them together into a single tibble of summaries using the `bind_rows()` function from `dplyr`.

```{R}
models <- map(formulas, ~lm(.x, data = df))
summaries <- map(models, broom::glance) %>% bind_rows()
summaries
```



---


###### 4.4  (5 points)

Extract the `adj.r.squared` values from `summaries` and use them to identify the formula with the _**highest**_ adjusted R-squared value.

```{R}
adj_r_squared <- summaries$adj.r.squared
highest_r2 <- formulas[[which.max(adj_r_squared)]]
highest_r2
```

Store resulting formula as a variable called `rsq_formula`.

```{R}
rsq_formula <- formula(highest_r2)
```


---

###### 4.5  (5 points)

Extract the `AIC` values from `summaries` and use them to identify the formula with the **_lowest_** AIC value.


```{R}
values_aic <- summaries$AIC
lowest_aic <- formulas[[which.min(values_aic)]]
lowest_aic
```

Store resulting formula as a variable called `aic_formula`.


```{R}
aic_formula <- formula(lowest_aic)
```

---

###### 4.6  (15 points)

Combine all formulas shortlisted into a single vector called `final_formulas`.

```{R}
null_formula <- formula(null_model)
full_formula <- formula(full_model)
final_formulas <- c(
  null_formula,
  full_formula,
  backward_formula,
  forward_formula,
  lasso_formula, 
  ridge_formula,
  rsq_formula,
  aic_formula
)

#final_formulas
```

* Are `aic_formula` and `rsq_formula` the same? How do they differ from the formulas shortlisted in question 3?

It is clear that both formulas aren't the same. As a first instance, the aic_formula do not select the same variables as the rsq_formula. One of the main reasons is because the rsq_formula takes the same formula as forward method and backwards method. Both methods look through all the combination of predictors in order to find the lowest AIC and highes r squared. While the other formulas in question 3 where only searching through all possible subsets.

* Which of these is more reliable? Why? 

I personally believe that the aic_formula is more reliable thant the rsq_formula since this formula focuses on finding the smallest aic causing the model correctly fit the data. So it is the most optimal. I corroborated this by testing.

* If we had a dataset with $10,000$ columns, which of these methods would you consider for your analyses? Why?

I would personally choose the lasso or ridge regression method since they use penalty for the predictors used. Since we have a dataset with very large number of columns, other models would really take a lot of time to deliver an output and a conclusion. Finally, it is important to state that these methods can work with very large datasets and provide high correlation on their output.

---

###### 4.7  (10 points)


Use `map()` and `glance()` to extract the `sigma, adj.r.squared, AIC, df`, and `p.value` statistics for each model obtained from `final_formulas`. Bind them together into a single data frame `summary_table`. Summarize your main findings.

```{R}
# summary_table <- map(
#   final_formulas,
#   ~glance(lm(x, df)) %>% 
#     select(sigma, adj.r.squared, AIC, df, p.value)
# ) %>% bind_rows()
# summary_table %>% knitr::kable()
```

For some reason the function is not working for me or I'm not sure if it is entirely correct but I'm leaving what I did just in case there is an error with the platform I'm using I will contact the professor about it when he is avaliable.



:::{.hidden unless-format="pdf"}
\pagebreak
:::

<br><br><br><br>
<br><br><br><br>
---


# Appendix


#### Convenience function for creating a formula object

The following function which takes as input a vector of column names `x` and outputs a `formula` object with `quality` as the response variable and the columns of `x` as the covariates. 

```R
make_formula <- function(x){
  as.formula(
    paste("quality ~ ", paste(x, collapse = " + "))
  )
}
# For example the following code will
# result in a formula object
# "quality ~ a + b + c"
make_formula(c("a", "b", "c"))
```

#### Convenience function for `glmnet`

The `make_model_matrix` function below takes a `formula` as input and outputs a **rescaled** model matrix `X` in a format amenable for `glmnet()`

```R
make_model_matrix <- function(formula){
  X <- model.matrix(formula, df)[, -1]
  cnames <- colnames(X)
  for(i in 1:ncol(X)){
    if(!cnames[i] == "typewhite"){
      X[, i] <- scale(X[, i])
    } else {
      colnames(X)[i] <- "type"
    }
  }
  return(X)
}
```




::: {.callout-note collapse="true"}
## Session Information

Print your `R` session information using the following command

```{R}
sessionInfo()
```
:::
